{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44155751",
   "metadata": {},
   "source": [
    "## About the necessity of careful programming. \n",
    "\n",
    "**Note:** In this tutorial we will consider $m_o$  and  $\\hat{\\boldsymbol{\\mu}}$ to be $(K \\times 1)$ where $dim(\\boldsymbol{\\theta})  = K$\n",
    "\n",
    "\n",
    "Two of the operations required by the Splines'n Lines method, for $o = 1,2,..., O$, are given by \n",
    "\n",
    "$$ C_o = (\\hat{\\Sigma}^{-1} + \\textbf{X}_o^T \\textbf{T}_o \\textbf{X}_o  )^{-1} $$\n",
    "\n",
    "$$m_o = C_o(\\textbf{X}_o \\textbf{T}_o \\textbf{y}_o + \\hat{\\Sigma}^{-1} \\hat{\\boldsymbol{\\mu}}) $$\n",
    "\n",
    "$$\\hat{ \\Sigma } = \\sum_{o =1}^{O} \\Big(  C_o + m_om_o^T - \\hat{\\boldsymbol{\\mu}}\\hat{\\boldsymbol{\\mu}}^T  \\Big)$$\n",
    "\n",
    "\n",
    "Thus, in principle, one naive algorithm to obtain the $C_o$, $m_o$ and ultimately $\\hat{\\Sigma}^{-1}$ is the following \n",
    "\n",
    "1. **for** $o = 1,2,..., O$ **do:** \n",
    "\n",
    "    1.1 $C_o \\gets (\\hat{\\Sigma}^{-1} + \\textbf{X}_o^T \\textbf{T}_o \\textbf{X}_o  )^{-1}$ \n",
    "\n",
    "    2.1 $m_o \\gets C_o(\\textbf{X}_o \\textbf{T}_o \\textbf{y}_o + \\hat{\\Sigma}^{-1} \\hat{\\boldsymbol{\\mu}}) $\n",
    "2. $\\hat{ \\Sigma } \\gets \\sum_{o =1}^{O} \\Big(  C_o + m_om_o^T - \\hat{\\boldsymbol{\\mu}}\\hat{\\boldsymbol{\\mu}}^T  \\Big)$\n",
    "\n",
    "\n",
    "In what follows we will explore some reasons for which implementing the previous algorithm and obtaining the components $C_o$, $m_o$ isnt as simple as it seems. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896ce9d4",
   "metadata": {},
   "source": [
    "### Memory usage in a computer and the power of numerical computin libraries. \n",
    "\n",
    "The purpose of the following videos is too illustrate the way that memory management works and, as a result, how can it affect algorithmic efficiency and \n",
    "execution time. \n",
    "\n",
    "Watch the following video up to the end of the Pointers section: https://www.youtube.com/watch?v=kcRdFGbzR1I&t=947s \n",
    "Watch the following video up to minute 40: https://www.youtube.com/watch?v=X8h4dq9Hzq8&t=1458s\n",
    "Watch the following video  https://www.youtube.com/watch?v=QXjU9qTsYCc \n",
    "\n",
    "**Task: Matrix multiplication** Create a code that recieves two numpy np.ndarray type of objects $A$ and $B$, checks whether their dimensions match for multiplication and returns a third np.ndarray object $C$ whose entries equal to those of $AB$. \n",
    "\n",
    "You are **not** allowed to use the numpy commands/functions @, np.dot, A*B. \n",
    "\n",
    "You **are** allowed to use the function multiply\\_vectors\\_entrywise a and the np.sum function. Observe that A[i, :]@B[:, j] =np.sum( mutiply_vectors_entrywise(Af[i, :],Bf[:, j] )) where Af[i,:] and Bf[:,j] are the flattened versions of A[i, :] and B[:, j].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "\"\"\"\n",
    "Hint: Remember you can check the dimensions of a np.ndarray with A.shape \n",
    "Note, some good practices: \n",
    "When coding, it is a good practice to document what it does. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "mutiply_vectors_entrywise \n",
    "recieves two numpy arrays a and b of equal length and returns a new numpy ndarray c with values a[i]*b[i]\n",
    "\"\"\"\n",
    "\n",
    "def multiply_vectors_entrywise(a,b):\n",
    "    assert len(a) == len(b) #Checks whether the \n",
    "    c =  np.zeros(len(a)) #Creates new vector. \n",
    "\n",
    "    for i in range(len(c)):\n",
    "        c[i] =   #Obtains i-th entry of c. \n",
    "    return c \n",
    "\n",
    "\n",
    "\n",
    "def multiply_matrices(A,B):\n",
    "    assert (type(A) == np.ndarray) and (type(B) == np.ndarray), \"Fatal error! either A or B arent of type np.ndarray\" #Checks whether A and B are of type np.ndarray. \n",
    "\n",
    "    a_r, a_c = A.shape #Gets the dimensions of A (a_r x a_c)\n",
    "    b_r, b_c = B.shape #Gets the dimensions of B (b_r x b_c)\n",
    "\n",
    "    assert  a_c == b_r, \"Fatal error!, columns of A dont match the rows of B\" #Checks if columns of A (a_c) match the rows of B (b_r)\n",
    "    C = np.zeros((a_r, b_c)) #Creates new np.ndarray of dimensions a_r x b_c \n",
    "\n",
    "    #This iterated loop obtains the entries of C (e.g. C[i,j]). \n",
    "    for i in range(a_r):\n",
    "        for j in range(b_c):\n",
    "            C[i,j] = #Multiples the row i of A times the column j of B\n",
    "\n",
    "    return C \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ae0917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.7039410540814947e-14\n",
      "Test number 0 passed ! \n",
      "3.061317943962309e-14\n",
      "Test number 1 passed ! \n",
      "2.847716647714797e-14\n",
      "Test number 2 passed ! \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "One very important practice is to check whether your code is actually doing what you think it is doing. \n",
    "To do so, it is important to give it particular cases (to which you know the answers to) and check whether your \n",
    "code is capable of replicating them. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "test_cases = 3\n",
    "tol = 1e-10 #Gets tolerance for difference. \n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(201763) #Tip: When dealing with computations which involve (semi)random number generations it is a good practice to \n",
    "                        #set a seed. This will ensure that, every time you run a single script, all of the random components remain the same. \n",
    "                        #This is essential for replicability in academic settings and for checking the results of your code. \n",
    "\n",
    "\n",
    "for t in range(test_cases):\n",
    "\n",
    "    #Generates matrix dimensions at random, ensuring that they are capable of being multiplied. \n",
    "    a_r = np.random.randint( 2, 10 )\n",
    "    a_c = np.random.randint( 2, 10)\n",
    "    b_r = a_c \n",
    "    b_c =np.random.randint( 2, 10)\n",
    "\n",
    "    #Generates matrices at random to being multiplied. \n",
    "    A = np.random.uniform(0, 20, size = (a_r, a_c)) #\n",
    "    B = np.random.uniform(-10,10, size = (b_r, b_c))\n",
    "\n",
    "    C = A@B \n",
    "    C_mine = multiply_matrices(A, B)\n",
    " \n",
    "    if np.linalg.norm( C - C_mine )/a_r*b_c < tol: #Checks whether C and C_mine are sufficiently simila by taking their norm. \n",
    "        \n",
    "        print(f\"Test number {t} passed ! \")\n",
    "    else:\n",
    "\n",
    "        print(f\"Test number {t} NOT passed ! \")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Question to answer: Why couldnt we just check similarity between C and C_mine by using C == C_mine? (hint. the answer is on the first video of the following section)\n",
    "why is it necessary to set a tolerance level?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46afbcae",
   "metadata": {},
   "source": [
    "Now, lets compare how our code compares with numpy's implementation by taking very large matrices. Our comparision metrics will be execution time, and number of allocations. \n",
    "\n",
    "\n",
    "**Task:** Complete the following code so it incudes execution time comparisons. Compare the results. \n",
    "\n",
    "\n",
    "**Important** This will run for about an hour. I recommend only to analyze it and learn te appropiate lessons. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30a25703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going for 0\n",
      "Going for 1\n",
      "Going for 2\n",
      "Going for 3\n",
      "Going for 4\n",
      "Going for 5\n",
      "Going for 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(201763)\n",
    "import tracemalloc \n",
    "import time \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "a_r_v = 20*(np.arange(7) + 1)\n",
    "\n",
    "a_c_v = 20*(np.arange(7) + 1)\n",
    "\n",
    "b_c_v = 20*(np.arange(7) + 1)\n",
    "\n",
    "num_cases = 10\n",
    "\n",
    "mean_v_mine= []\n",
    "mean_s_mine = []\n",
    "mean_v_np  = []\n",
    "mean_s_np = []\n",
    "mean_time_m = []\n",
    "mean_time_np = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(a_r_v)):\n",
    "    print(f\"Going for {i}\")\n",
    "    c_mine = []\n",
    "    s_mine = []\n",
    "    c_np = []\n",
    "    s_np = []\n",
    "\n",
    "\n",
    "    for t in range(num_cases):\n",
    "        A = np.random.uniform(-10, 10, size = (a_r_v[i], a_c_v[i]))\n",
    "        B = np.random.uniform(-10, 10, size = (a_c_v[i], b_c_v[i]))\n",
    "\n",
    "        tracemalloc.start()\n",
    "        C = A@B \n",
    "        np_snapshot = tracemalloc.take_snapshot()\n",
    "        tracemalloc.stop()\n",
    "        stats = np_snapshot.statistics('traceback')\n",
    "\n",
    "        c_np.append(sum(stat.count for stat in stats )) #Adds total number of allocations. \n",
    "        s_np.append(sum(stat.size for stat in stats ) ) #Adds total number of bytes. \n",
    "\n",
    "\n",
    "        tracemalloc.start()\n",
    "        C = multiply_matrices(A,B)\n",
    "        np_snapshot = tracemalloc.take_snapshot()\n",
    "        tracemalloc.stop()\n",
    "        stats = np_snapshot.statistics('traceback')\n",
    "\n",
    "        c_mine.append(sum(stat.count for stat in stats )) #Adds total number of allocations. \n",
    "        s_mine.append(sum(stat.size for stat in stats ) ) #Adds total number of bytes. \n",
    "    \n",
    "    mean_v_np.append(np.mean(c_np))\n",
    "    mean_s_np.append(np.mean(s_np))\n",
    "    mean_v_mine.append(np.mean(c_mine))\n",
    "    mean_s_mine.append(np.mean(s_mine))\n",
    "\n",
    "\n",
    "plt.plot(np.arange(len(a_r_v)), mean_v_np, color = 'blue') #mean number of allocations numpy (blue)\n",
    "plt.plot(np.arange(len(a_r_v)), mean_v_mine, color = 'red') #mean number of allocations mine (red)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b29ca48",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "It is worth asking, if both functions give exactly the same output,  **why is one much faster than the other?**\n",
    "\n",
    "\n",
    "Some reasons include\n",
    "1. Numpy does not execute its commands in python, but in C. This allows numpy to have a higher control over memory management. \n",
    "2. Numpy often does computation with the indices of a multidimensional array. That is, in many instances it is possible to know which operation (in our case multiplication) will involve which indices in the original matrices $A$ and $B$ and how they relate to the final output $C$.\n",
    "3. Following 2., our code defines temporary variables which are modified. Often, defining temporary variables is quite costly in terms of computing time and memory allocation. We may think about it in the following way: If for each iteration of a loop (e.g. a for or a while) we define a new variable a, in each of the flops the code will re-allocate a memory spot for a, modify it and delete it. \n",
    "\n",
    "\n",
    "Thus, one of the lessons to learn is that when dealing with large scale computing, **the output isnt the only thing that matters, but how that output is obtained matters as well.** It is always crucial to remember that, something that is not well programmed will be a multiplied problem when trying to scale it. \n",
    "\n",
    "\n",
    " Some general tips include: \n",
    "\n",
    "1. Whenever possible, delegate numerical tasks to specialized libraries like scipy, numpy and torch. (Our code relies heavely on the three of them).\n",
    "2. Try to store things in the most efficient way possible. For example, if we are going to use matrices $A_1$, $A_2$, ..., $A_N$, all of dimension $K \\times M$ it is worth to use a multidimensional array instead of $N$ individual matrices. Examples include np.ndarray's and torch tensors. i.e. \n",
    "\n",
    "$Matrices = [A1, A2, ..., AN]$ is going to be way less memory efficient than a $(N \\times K \\times M)$ np.ndarray or a torch tensor of the same dimensions. \n",
    "\n",
    "\n",
    "Libraries like torch have optimized functions for tensors of this type. \n",
    "\n",
    "3. When possible, pre-allocate the memory to be utilized for a particular case. Following the previous example, we may initialize a $(N \\times K \\times M)$ tensor and, when seeking to access a particular matrix $An$ simply call $A[n, :, :]$ or, if we were to modify a particular entry $An[i,j]$ use $A[n,i,j]$. \n",
    "\n",
    "\n",
    "4. If there are terms to be used multiple times, compute them once and call them in each iteration. For example, consider the task of computing $X_o^T T_o l_o$ where $X_o$ and $T_o$ are fixed, but $l_o$ changes per iteration. Then, calculating $A = X_o^TT_o$ and using it to calculate $A l_o$ is most likely a good way to save on computing time rather than computing $X_o^T T_o$ in each iteration. \n",
    "\n",
    "5. Whenever you are writting code that needs to scale, do not think on what a particular line of code gives as output, but think on the intermediary processes which need to ocurr for that output to ocurr. \n",
    "\n",
    "6. Whenever possible, try using theoretical properties of objects (e.g. matrices) to obtain desired outputs without using necessarely the theoretical operations. e.g. a matrix $A = diag(1, 2,3)$ multiplied by a matrix $B$ of dimension $ 3 \\times L$  (AB) will result on $row_1(AB) = row_1(B) $, $row_2(AB) = 2row_2(B)$ and $row_3(AB) = 3row_3(B)$. So, it may be way more efficient to obtain $row_1(B)$, $2row_2(B)$ and $3row_3(B)$ rather than doing the entire multiplication. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62661058",
   "metadata": {},
   "source": [
    "### Vectorization and parallel computing \n",
    "\n",
    "\n",
    "To understand why some torch and numpy functions are so optimized (and other functions in other programing languages like R or Julia) we must have a basic notion of what parallel computing is and how does it compare to concurrent computing. Watch the following videos:\n",
    "\n",
    "1.  https://www.youtube.com/watch?v=q7sgzDH1cR8 \n",
    "2. https://www.youtube.com/watch?v=pPStdjuYzSI\n",
    "3. https://www.youtube.com/watch?v=BR3Qx9AVHZE\n",
    "\n",
    "\n",
    "\n",
    "Now, having seen the basics of parallel computing there is an important lesson to highlight: **Doing tasks in parallel is not always worth it and, in fact, it can be counterproductive**. If a process is performed in parallel, the output of the parallel processes must be \"re-collected\" at some point. If parallel instructions are abused, it may ocurr that re-collecting the pieces might be more expensive than doing things with concurrent computing. \n",
    "\n",
    "An additional warning with parallel computing is that, **parallel computing may not be performed if there are pieces that depend on each other**. Consider the case of defining a random walk defined by \n",
    "\n",
    "$$ X_{n+1} = X_n + \\varepsilon_n$$\n",
    "\n",
    "\n",
    "with $\\varepsilon_n \\sim \\mathcal{N}(0, X_n^2/2^n )$ it is clear that we cannot parallelize this process as a step $n+1$ depends on the step $n$ which itself depends on $n-1$ and so on. This is often the case with iterative processes like MCMC walks and iterative numerical procedures. What could be done is that, if we need $K$ random walks, we could do each walk in parallel and, within each parallel process, do the recursive definition in a concurrent fashion. i.e. each individual random walk is computed independently of the other, but all are computed at the same time in parallel. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03771f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[8.1271],\n",
      "         [3.0432],\n",
      "         [1.4977],\n",
      "         [1.2124],\n",
      "         [5.8332],\n",
      "         [0.3984]],\n",
      "\n",
      "        [[1.9337],\n",
      "         [9.9870],\n",
      "         [0.8658],\n",
      "         [3.6691],\n",
      "         [8.4442],\n",
      "         [8.5502]],\n",
      "\n",
      "        [[7.1806],\n",
      "         [0.4623],\n",
      "         [4.6823],\n",
      "         [2.6555],\n",
      "         [1.9176],\n",
      "         [7.7613]]], dtype=torch.float64)\n",
      "tensor([[[8.1271, 3.0432, 1.4977, 1.2124, 5.8332, 0.3984]],\n",
      "\n",
      "        [[1.9337, 9.9870, 0.8658, 3.6691, 8.4442, 8.5502]],\n",
      "\n",
      "        [[7.1806, 0.4623, 4.6823, 2.6555, 1.9176, 7.7613]]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[[66.0503, 24.7323, 12.1718,  9.8529, 47.4075,  3.2377],\n",
      "         [24.7323,  9.2609,  4.5577,  3.6894, 17.7516,  1.2124],\n",
      "         [12.1718,  4.5577,  2.2430,  1.8157,  8.7363,  0.5967],\n",
      "         [ 9.8529,  3.6894,  1.8157,  1.4698,  7.0719,  0.4830],\n",
      "         [47.4075, 17.7516,  8.7363,  7.0719, 34.0267,  2.3239],\n",
      "         [ 3.2377,  1.2124,  0.5967,  0.4830,  2.3239,  0.1587]],\n",
      "\n",
      "        [[ 3.7393, 19.3122,  1.6742,  7.0950, 16.3288, 16.5338],\n",
      "         [19.3122, 99.7410,  8.6465, 36.6431, 84.3327, 85.3916],\n",
      "         [ 1.6742,  8.6465,  0.7496,  3.1766,  7.3108,  7.4026],\n",
      "         [ 7.0950, 36.6431,  3.1766, 13.4620, 30.9824, 31.3714],\n",
      "         [16.3288, 84.3327,  7.3108, 30.9824, 71.3047, 72.2000],\n",
      "         [16.5338, 85.3916,  7.4026, 31.3714, 72.2000, 73.1065]],\n",
      "\n",
      "        [[51.5612,  3.3199, 33.6215, 19.0680, 13.7695, 55.7307],\n",
      "         [ 3.3199,  0.2138,  2.1648,  1.2278,  0.8866,  3.5884],\n",
      "         [33.6215,  2.1648, 21.9236, 12.4337,  8.9787, 36.3404],\n",
      "         [19.0680,  1.2278, 12.4337,  7.0516,  5.0921, 20.6099],\n",
      "         [13.7695,  0.8866,  8.9787,  5.0921,  3.6772, 14.8830],\n",
      "         [55.7307,  3.5884, 36.3404, 20.6099, 14.8830, 60.2375]]],\n",
      "       dtype=torch.float64)\n",
      "tensor(3.1477e-06, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "What do the slices m0_outer[o, :, :] represent? Simulate possible values for m0 and compare the two ways of obtaining m0_outer. Compare the outputs numerically, find out the way torch.bmm works, and \n",
    "explain why the individual outputs are obtained. \n",
    "\"\"\"\n",
    "import torch \n",
    "import numpy as np \n",
    "\n",
    "\n",
    "O = 3\n",
    "B = 6\n",
    "\n",
    "np.random.seed(201763)\n",
    "\n",
    "m0 = torch.zeros(O, B)\n",
    "\n",
    "m0 = torch.from_numpy(np.random.uniform(0, 10, size = (O, B)))\n",
    "\n",
    "\n",
    "#Form 1, with torch.bmm\n",
    "m0_outer1 = torch.bmm(m0.unsqueeze(-1), m0.unsqueeze(-2)   )\n",
    "#Form 2, naive way. Note that we are still applying the good practice of pre-allocating memory. \n",
    "m0_outer2 = torch.zeros(O, B, B) #Creates a torch tensor to store outer products of m0's. \n",
    "\n",
    "for o in range(O): #Gets the outer product of individual mean estimates. \n",
    "    m0_outer2[o,:,:] = m0[o,:].unsqueeze(-1)@m0[o,:].unsqueeze(-2)#Gets inner product of the o-th mean estimate.\n",
    "\n",
    "\n",
    "diff = torch.abs(m0_outer1 - m0_outer2) \n",
    "\n",
    "print(torch.max(diff))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e1c83d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2., 0., 0., 0.],\n",
      "         [0., 2., 0., 0.],\n",
      "         [0., 0., 2., 0.],\n",
      "         [0., 0., 0., 2.]],\n",
      "\n",
      "        [[3., 0., 0., 0.],\n",
      "         [0., 3., 0., 0.],\n",
      "         [0., 0., 3., 0.],\n",
      "         [0., 0., 0., 3.]],\n",
      "\n",
      "        [[4., 0., 0., 0.],\n",
      "         [0., 4., 0., 0.],\n",
      "         [0., 0., 4., 0.],\n",
      "         [0., 0., 0., 4.]],\n",
      "\n",
      "        [[5., 0., 0., 0.],\n",
      "         [0., 5., 0., 0.],\n",
      "         [0., 0., 5., 0.],\n",
      "         [0., 0., 0., 5.]],\n",
      "\n",
      "        [[6., 0., 0., 0.],\n",
      "         [0., 6., 0., 0.],\n",
      "         [0., 0., 6., 0.],\n",
      "         [0., 0., 0., 6.]]])\n",
      "tensor([[[0.5000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.5000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.5000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.5000]],\n",
      "\n",
      "        [[0.3333, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.3333, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.3333, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.3333]],\n",
      "\n",
      "        [[0.2500, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.2500, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.2500, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.2500]],\n",
      "\n",
      "        [[0.2000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.2000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.2000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.2000]],\n",
      "\n",
      "        [[0.1667, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.1667, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.1667, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.1667]]])\n",
      "tensor([[[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.]],\n",
      "\n",
      "        [[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.]],\n",
      "\n",
      "        [[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.]],\n",
      "\n",
      "        [[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.]],\n",
      "\n",
      "        [[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.]]])\n",
      "tensor([[[3., 1., 1., 1.],\n",
      "         [1., 3., 1., 1.],\n",
      "         [1., 1., 3., 1.],\n",
      "         [1., 1., 1., 3.]],\n",
      "\n",
      "        [[4., 1., 1., 1.],\n",
      "         [1., 4., 1., 1.],\n",
      "         [1., 1., 4., 1.],\n",
      "         [1., 1., 1., 4.]],\n",
      "\n",
      "        [[5., 1., 1., 1.],\n",
      "         [1., 5., 1., 1.],\n",
      "         [1., 1., 5., 1.],\n",
      "         [1., 1., 1., 5.]],\n",
      "\n",
      "        [[6., 1., 1., 1.],\n",
      "         [1., 6., 1., 1.],\n",
      "         [1., 1., 6., 1.],\n",
      "         [1., 1., 1., 6.]],\n",
      "\n",
      "        [[7., 1., 1., 1.],\n",
      "         [1., 7., 1., 1.],\n",
      "         [1., 1., 7., 1.],\n",
      "         [1., 1., 1., 7.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np \n",
    "\n",
    "O = 5\n",
    "B = 4\n",
    "\n",
    "\n",
    "S = torch.zeros(O,B,B)\n",
    "for o in range(O):\n",
    "    S[o,:,:] = (o+2)*torch.from_numpy(np.eye(B))\n",
    "\n",
    "\n",
    "Si = torch.linalg.inv(S)\n",
    "\n",
    "\n",
    "print(S)\n",
    "print(Si)\n",
    "\n",
    "print(torch.bmm(S, Si))\n",
    "\n",
    "\n",
    "print(S + torch.ones(B, B ))\n",
    "\n",
    "#print(torch.bmm(S, (torch.arange(B)+1).reshape(-1,1) ))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
